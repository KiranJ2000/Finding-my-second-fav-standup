# -*- coding: utf-8 -*-
"""ok1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17cp7K__RN3P92RR74gwrrKeVNf3DNrXz
"""

#!pip install transformers

import transformers

bert = transformers.BertModel.from_pretrained('bert-base-uncased')

tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')

sentence = 'cut my life into pieces this is my last resort jayashree is a good boy'
ok = 'suffocation no breathing dont give a fuck '

inputs = tokenizer.encode_plus(sentence,ok,True,30,pad_to_max_length=True,return_attention_mask=True,return_token_type_ids=True)

print(sentence)
print(ok)
print('\n')
print(inputs['input_ids'])
print('\n')
print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))
print('\n')
print(inputs['token_type_ids'])
print('\n')
print(inputs['attention_mask'])

import torch

x = torch.tensor([[2.,2.],[2.,2.]],requires_grad=True)
out = x.pow(2).sum()
out.backward()
x.grad

import torch


class BERTDataset:
  
    def __init__(self, review, target):
        self.review = review
        self.target = target
        self.tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')
        self.max_len = 40

    def __len__(self):
        return len(self.review)

    def __getitem__(self, item):
        review = str(self.review[item])
        review = " ".join(review.split())

        inputs = self.tokenizer.encode_plus(
            review,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            pad_to_max_length=True,
        )

        ids = inputs["input_ids"]
        mask = inputs["attention_mask"]
        token_type_ids = inputs["token_type_ids"]

        return review[item]

        #return {
         #   "ids": torch.tensor(ids, dtype=torch.long),
         #   "mask": torch.tensor(mask, dtype=torch.long),
         #   "token_type_ids": torch.tensor(token_type_ids, dtype=torch.long),
         #   "targets": torch.tensor(self.target[item], dtype=torch.float),
        #}

hey = BERTDataset(['hey jude dont get me go','what is going on something to do'],[1,2])

train_data_loader = torch.utils.data.DataLoader(
        hey, batch_size=1, num_workers=1
    )

train_data_loader

for i in train_data_loader:
  print(i)

for bi, d in enumerate(train_data_loader):
  print(d)

from ok import BERTDataset

